{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gmflau/gen-ai/blob/main/AWS_Bedrock_LangChain_Redis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2-i8jBl9GRH"
      },
      "source": [
        "# Document Question Answering with Langchain, AWS Bedrock and Redis\n",
        "\n",
        "![Redis](https://redis.com/wp-content/themes/wpx/assets/images/logo-redis.svg?auto=webp&quality=85,75&width=120)\n",
        "\n",
        "This notebook would use your own deployment of AWS Bedrock, Redis with Vector Similarity Search and Langchain to answer guestions about the information contained in a document."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install packages and import modules"
      ],
      "metadata": {
        "id": "UhopMfshrQ9x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install packages\n",
        "!python3 -m pip install -qU redis \"langchain==0.0.227\" boto3 wget\n",
        "\n",
        "# import modules\n",
        "import os\n",
        "import wget\n",
        "import boto3\n",
        "import json\n",
        "from getpass import getpass\n",
        "from langchain.embeddings.bedrock import BedrockEmbeddings\n",
        "from langchain.llms import Bedrock\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.vectorstores.redis import Redis\n",
        "from langchain.text_splitter import CharacterTextSplitter"
      ],
      "metadata": {
        "id": "rA8EAkVGq8Np"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: boto3 is part of AWS SDK for Python and is required to use Bedrock LLM"
      ],
      "metadata": {
        "id": "ud7YxpFgrYA0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Init Bedrock client\n",
        "\n",
        "To authorize in AWS service we can use `~/.aws/config` file with [configuring credentials](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html#configuring-credentials) or pass `AWS_ACCESS_KEY`, `AWS_SECRET_KEY`, `AWS_REGION` to boto3 module.\n",
        "\n",
        "We're using second approach for our example."
      ],
      "metadata": {
        "id": "W23_VB0Crb72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "default_region = \"us-east-1\"\n",
        "AWS_ACCESS_KEY = getpass(\"AWS Acces key: \")\n",
        "AWS_SECRET_KEY = getpass(\"AWS Secret key: \")\n",
        "AWS_REGION = input(f\"AWS Region [default: {default_region}]: \") or default_region\n",
        "\n",
        "bedrock_client = boto3.client(\n",
        "    service_name=\"bedrock-runtime\",\n",
        "    region_name=AWS_REGION,\n",
        "    aws_access_key_id=AWS_ACCESS_KEY,\n",
        "    aws_secret_access_key=AWS_SECRET_KEY\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dDvbrasrawn",
        "outputId": "828f5a93-b7e1-41ac-c2d1-bfcbdd6d636e"
      },
      "execution_count": 62,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AWS Acces key: ··········\n",
            "AWS Secret key: ··········\n",
            "AWS Region [default: us-east-1]: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBlbUrB27QQs"
      },
      "source": [
        "### Install Redis Stack\n",
        "\n",
        "Redis will be used as Vector Similarity Search engine for Langchain. Instead of using in-notebook Redis Stack you can provision your own free instance of Redis in the cloud. Get your own Free Redis Cloud instance at https://redis.com/try-free/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKMKXPY2j8Gt",
        "outputId": "f478a460-afc8-4e76-9d93-9d3f6594d751"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "deb [signed-by=/usr/share/keyrings/redis-archive-keyring.gpg] https://packages.redis.io/deb jammy main\n",
            "Starting redis-stack-server, database path /var/lib/redis-stack\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "gpg: cannot open '/dev/tty': No such device or address\n",
            "curl: (23) Failed writing body\n"
          ]
        }
      ],
      "source": [
        "%%sh\n",
        "curl -fsSL https://packages.redis.io/gpg | sudo gpg --dearmor -o /usr/share/keyrings/redis-archive-keyring.gpg\n",
        "echo \"deb [signed-by=/usr/share/keyrings/redis-archive-keyring.gpg] https://packages.redis.io/deb $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/redis.list\n",
        "sudo apt-get update  > /dev/null 2>&1\n",
        "sudo apt-get install redis-stack-server  > /dev/null 2>&1\n",
        "redis-stack-server --daemonize yes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7UsU1Ts7TUL"
      },
      "source": [
        "### Connect to Redis\n",
        "\n",
        "By default this notebook would connect to the local instance of Redis Stack. If you have your own Redis Cloud instance - replace REDIS_PASSWORD, REDIS_HOST and REDIS_PORT values with your own."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "dyPfCO3pkB7M"
      },
      "outputs": [],
      "source": [
        "import redis\n",
        "import os\n",
        "\n",
        "\n",
        "REDIS_HOST = os.getenv(\"REDIS_HOST\", \"localhost\")\n",
        "REDIS_PORT = os.getenv(\"REDIS_PORT\", \"6379\")\n",
        "REDIS_PASSWORD = os.getenv(\"REDIS_PASSWORD\", \"\")\n",
        "#Replace values above with your own if using Redis Cloud instance\n",
        "#REDIS_HOST=\"redis-12996.c279.us-central1-1.gce.cloud.redislabs.com\"\n",
        "##REDIS_PORT=12996\n",
        "#REDIS_PASSWORD=\"xnurcS28JREs9S8HHemx2cKc1jLFi4ua\"\n",
        "\n",
        "#shortcut for redis-cli $REDIS_CONN command\n",
        "if REDIS_PASSWORD!=\"\":\n",
        "  os.environ[\"REDIS_CONN\"]=f\"-h {REDIS_HOST} -p {REDIS_PORT} -a {REDIS_PASSWORD} --no-auth-warning\"\n",
        "else:\n",
        "  os.environ[\"REDIS_CONN\"]=f\"-h {REDIS_HOST} -p {REDIS_PORT}\"\n",
        "\n",
        "REDIS_URL = f\"redis://:{REDIS_PASSWORD}@{REDIS_HOST}:{REDIS_PORT}\"\n",
        "INDEX_NAME = f\"qna:idx\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "9XL4P-k1d0Pi",
        "outputId": "0f097f3d-fa7f-4bfd-cb68-d680b82186f7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'state_of_the_union (3).txt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 65
        }
      ],
      "source": [
        "#!wget https://raw.githubusercontent.com/hwchase17/chat-your-data/master/state_of_the_union.txt\n",
        "wget.download(\"https://raw.githubusercontent.com/hwchase17/chat-your-data/master/state_of_the_union.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXDbABtw-vAQ"
      },
      "source": [
        "### Load text and split it into managable chunks\n",
        "\n",
        "Without this step any large body of text would exceed the limit of tokens you can feed to the LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "4zfOlQeeZjsN"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "loader = TextLoader(\"./state_of_the_union.txt\")\n",
        "documents = loader.load()\n",
        "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=30)\n",
        "texts = text_splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dv1JVm31_OF9"
      },
      "source": [
        "### Initialize embeddings engine\n",
        "\n",
        "Here we are using AWS Bedrock embeddings engine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "gaJrhuKa_Mwt"
      },
      "outputs": [],
      "source": [
        "embeddings = BedrockEmbeddings(client=bedrock_client)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zgvqB6wCJWK"
      },
      "source": [
        "## Init Bedrock LLM\n",
        "\n",
        "Next, we will initialize Bedrock LLM. In the Bedrock instance, will pass `bedrock_client` and specific `model_id`: `amazon.titan-text-express-v1`, `ai21.j2-ultra-v1`, `anthropic.claude-v2`, `cohere.command-text-v14` or etc. You can see list of available base models on [Amazon Bedrock User Guide](https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids-arns.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "iQRyyWU0CNbJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5142e4d8-d51e-4929-aeeb-9b64cf5a9606"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AWS model [default: amazon.titan-text-express-v1]: \n"
          ]
        }
      ],
      "source": [
        "default_model_id = \"amazon.titan-text-express-v1\"\n",
        "AWS_MODEL_ID = input(f\"AWS model [default: {default_model_id}]: \") or default_model_id\n",
        "llm = Bedrock(\n",
        "    client=bedrock_client,\n",
        "    model_id=AWS_MODEL_ID\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qrj-jeGmBRTL"
      },
      "source": [
        "### Create vector store from the documents using Redis as Vector Database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "yY69FViAjNv1"
      },
      "outputs": [],
      "source": [
        "def get_vectorstore() -> Redis:\n",
        "    \"\"\"Create the Redis vectorstore.\"\"\"\n",
        "\n",
        "    try:\n",
        "        vectorstore = Redis.from_existing_index(\n",
        "            embedding=embeddings,\n",
        "            index_name=INDEX_NAME,\n",
        "            redis_url=REDIS_URL\n",
        "        )\n",
        "        return vectorstore\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # Load Redis with documents\n",
        "    vectorstore = Redis.from_documents(\n",
        "        documents=texts,\n",
        "        embedding=embeddings,\n",
        "        index_name=INDEX_NAME,\n",
        "        redis_url=REDIS_URL\n",
        "    )\n",
        "    return vectorstore\n",
        "\n",
        "\n",
        "redis = get_vectorstore()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdzQa112Bf2b"
      },
      "source": [
        "## Specify the prompt template\n",
        "\n",
        "PromptTemplate defines the exect text of the response that would be fed to the LLM. This step is optional, but the defaults usually work well for OpenAI and might fall short for other models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "yKKn2KKp3TQ4"
      },
      "outputs": [],
      "source": [
        "def get_prompt():\n",
        "    \"\"\"Create the QA chain.\"\"\"\n",
        "    from langchain.prompts import PromptTemplate\n",
        "    from langchain.chains import RetrievalQA\n",
        "\n",
        "    # Define our prompt\n",
        "    prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, say that you don't know, don't try to make up an answer.\n",
        "\n",
        "    This should be in the following format:\n",
        "\n",
        "    Question: [question here]\n",
        "    Answer: [answer here]\n",
        "\n",
        "    Begin!\n",
        "\n",
        "    Context:\n",
        "    ---------\n",
        "    {context}\n",
        "    ---------\n",
        "    Question: {question}\n",
        "    Answer:\"\"\"\n",
        "\n",
        "    prompt = PromptTemplate(\n",
        "        template=prompt_template,\n",
        "        input_variables=[\"context\", \"question\"]\n",
        "    )\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgEXBujxG1dO"
      },
      "source": [
        "### Putting it all together\n",
        "\n",
        "This is where the Langchain brings all the components together in a form of a simple QnA chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "RKNSP0zqZq98"
      },
      "outputs": [],
      "source": [
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=redis.as_retriever(search_type=\"similarity_distance_threshold\",search_kwargs={\"distance_threshold\":0.5}),\n",
        "    #return_source_documents=True,\n",
        "    chain_type_kwargs={\"prompt\": get_prompt()},\n",
        "    #verbose=True\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ox7LffJ8VNe"
      },
      "source": [
        "### Debugging Redis\n",
        "\n",
        "The code block below is exampleof how you can interact with the Redis Database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "rGH8mdG2na0w"
      },
      "outputs": [],
      "source": [
        "#!redis-cli $REDIS_CONN keys \"*\"\n",
        "#!redis-cli $REDIS_CONN HGETALL \"doc:qna:idx:2005b093c6af4f6c899779802910ed69\"\n",
        "\n",
        "###-- FLUSHDB will wipe out the entire database!!! Use with caution --###\n",
        "#!redis-cli $REDIS_CONN flushdb\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SURTtVbYBFGc"
      },
      "source": [
        "## Finally - let's ask questions!\n",
        "\n",
        "Examples:\n",
        "- What did the president say about Kentaji Brown Jackson\n",
        "- Did he mention Stephen Breyer?\n",
        "- What was his stance on Ukraine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0JkswfOHZu9h",
        "outputId": "9cffabd2-ff35-4e65-e156-d8e99c3e9131"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" I'm not familiar with the president's statements regarding Kentaji Brown Jackson.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 73
        }
      ],
      "source": [
        "query = \"What did the president say about Kentaji Brown Jackson\"\n",
        "qa.run(query)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}